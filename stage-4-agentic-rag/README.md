# Stage 4: Agentic RAG - Retrieval-Augmented Generation Agent

**Status**: âœ… Complete (Phase 1: Ingestion Pipeline + Phase 2: Conversational Agent)

## âš ï¸ Important: Use Python Embedding Service

**Due to an Ollama API bug, you MUST use the Python embedding service for this stage.**

The Python service provides:
- âœ… Reliable embedding generation
- âœ… Drop-in replacement for Ollama API
- âœ… Fast local inference
- âœ… No external dependencies once set up

### Setup Steps (Required)

**Step 1: Start the Python Embedding Service**

```bash
cd stage-4-agentic-rag/embedding-service
./start.sh
```

**What happens on first run:**
- Creates conda environment with Python 3.11
- Installs dependencies (fastapi, sentence-transformers, PyTorch, etc.)
- Downloads nomic-embed-text-v1.5 model (~500MB, one-time)
- Starts service on http://localhost:8001

**Subsequent runs:** Service starts immediately (environment and model cached)

**Wait for this output before proceeding:**
```
âœ“ Model loaded successfully (768 dimensions)
INFO:     Uvicorn running on http://0.0.0.0:8001
```

**Step 2: Run Ingestion (In a New Terminal)**

```bash
cd stage-4-agentic-rag
./ingest.sh
```

The ingestion script will:
- âœ“ Verify Python service is running
- âœ“ Start PostgreSQL with pgvector
- âœ“ Process documents and generate embeddings
- âœ“ Store everything in the database

### Prerequisites for Python Service

**Required:**
- **Conda** (Miniconda or Anaconda)
  - Install: https://docs.conda.io/en/latest/miniconda.html
- **8GB+ RAM** recommended
- **500MB disk space** for model download (one-time)

**Verify conda is installed:**
```bash
conda --version
# Should output: conda X.Y.Z
```

**If conda is not found:**
```bash
# Initialize conda for your shell
conda init bash  # or zsh, fish, etc.

# Restart your shell or source config
source ~/.bashrc  # or ~/.zshrc
```

### Troubleshooting Python Service

If you encounter issues, see [`embedding-service/README.md`](./embedding-service/README.md) for detailed troubleshooting.

**Quick verification:**
```bash
cd embedding-service
./verify-setup.sh
```

**Common issues:**
- **"conda: command not found"** â†’ Run `conda init bash` and restart shell
- **Wrong Python version** â†’ Environment will auto-create with Python 3.11
- **Port 8001 in use** â†’ Change port in `server.py`
- **Module not found** â†’ Run `./verify-setup.sh` to diagnose

---

## What This Stage Demonstrates

This stage showcases a production-ready RAG (Retrieval-Augmented Generation) system that combines:

### 1. **Document Ingestion Pipeline** (Shell Script)
- Uses pre-committed repository text files (no external dependencies by default)
- Optional fresh data fetching with `gitingest` (--refresh flag)
- Smart document chunking with overlap for context preservation
- Embedding generation via Ollama's `nomic-embed-text` model
- Vector storage in PostgreSQL with pgvector extension
- Idempotent operation (safe to re-run, only processes new/changed content)

### 2. **Conversational RAG Agent** (Java)
- Multi-turn conversations with memory retention
- JSON-based tool calling (Ollama native format)
- Context expansion via neighboring chunk retrieval
- Natural language Q&A over ingested documentation
- Full agent loop: Think â†’ Act (search) â†’ Observe â†’ Answer

## How RAG Works in This Stage

### The Problem: Grounding AI in Your Documentation

Large Language Models (LLMs) are trained on general internet data, but they don't know about:
- Your internal documentation
- Recent project updates
- Proprietary frameworks (like Embabel)
- Specific implementation details

RAG solves this by giving the LLM **relevant context** from your documentation at query time.

### The RAG Pipeline: From Code to Conversation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: INGESTION (One-Time Setup via ./ingest.sh)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Repository Content (committed text files)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Embabel Agent Repository        â”‚
   â”‚  â€¢ Java source files             â”‚
   â”‚  â€¢ Documentation (*.md)          â”‚
   â”‚  â€¢ Configuration files           â”‚
   â”‚  â€¢ Example code                  â”‚
   â”‚  (Pre-processed via gitingest)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
2. Document Chunking (DocumentChunker.java)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Split into ~800 token chunks    â”‚
   â”‚  with 200 token overlap          â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
   â”‚  Chunk 1: [tokens 0-800]         â”‚
   â”‚  Chunk 2: [tokens 600-1400] â†â”€â”  â”‚
   â”‚  Chunk 3: [tokens 1200-2000]   â”‚  â”‚
   â”‚           â†‘         â†‘          â”‚  â”‚
   â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â”‚           200 token overlap       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
3. Embedding Generation (EmbeddingService.java)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  For each chunk:                 â”‚
   â”‚  "@Agent annotation defines..."  â”‚
   â”‚           â†“                      â”‚
   â”‚  [Ollama: nomic-embed-text]      â”‚
   â”‚           â†“                      â”‚
   â”‚  [0.234, -0.891, 0.456, ...]     â”‚
   â”‚   â†‘                              â”‚
   â”‚   1024-dimensional vector        â”‚
   â”‚   (semantic meaning encoded)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
4. Vector Storage (PgVectorStore.java)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PostgreSQL + pgvector Extension         â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚ documents table                    â”‚  â”‚
   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
   â”‚  â”‚ id | source | chunk_index | ...   â”‚  â”‚
   â”‚  â”‚ embedding (vector[1024])           â”‚  â”‚
   â”‚  â”‚ content (text)                     â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â”‚                                           â”‚
   â”‚  IVFFlat Index for fast similarity searchâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Result: 487 searchable document chunks

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: QUERY TIME (Interactive via ./run.sh)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User Question
   "What is Embabel and how do I use it?"
                â”‚
                â–¼
2. RAG Agent Decides to Search
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  RAGAgent.java                   â”‚
   â”‚  â€¢ Analyzes question             â”‚
   â”‚  â€¢ Decides tool needed           â”‚
   â”‚  â€¢ Calls: search_documentation   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
3. Query Embedding
   "What is Embabel and how do I use it?"
                â”‚
                â–¼
   [Ollama: nomic-embed-text]
                â”‚
                â–¼
   [0.123, -0.567, 0.890, ...]
    â†‘
    Query vector (1024 dimensions)
                â”‚
                â–¼
4. Similarity Search
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PgVectorStore.search()                  â”‚
   â”‚  SELECT *, embedding <=> $1 AS distance  â”‚
   â”‚  FROM documents                          â”‚
   â”‚  ORDER BY distance                       â”‚
   â”‚  LIMIT 5;                                â”‚
   â”‚                                           â”‚
   â”‚  Cosine similarity finds closest vectors:â”‚
   â”‚  â€¢ Chunk 142: 0.89 similarity            â”‚
   â”‚  â€¢ Chunk 87:  0.85 similarity            â”‚
   â”‚  â€¢ Chunk 201: 0.82 similarity            â”‚
   â”‚  â€¢ Chunk 56:  0.79 similarity            â”‚
   â”‚  â€¢ Chunk 345: 0.76 similarity            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
5. Context Expansion (Optional)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  For each matched chunk:         â”‚
   â”‚  â€¢ Get previous chunk (N-1)      â”‚
   â”‚  â€¢ Get current chunk (N)         â”‚
   â”‚  â€¢ Get next chunk (N+1)          â”‚
   â”‚                                  â”‚
   â”‚  Ensures complete code examples  â”‚
   â”‚  and maintains narrative flow    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
6. Augmented Prompt
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  SYSTEM: You are a helpful assistant... â”‚
   â”‚                                          â”‚
   â”‚  CONTEXT (from vector search):           â”‚
   â”‚  ---                                     â”‚
   â”‚  [Chunk 142: Embabel is a framework...]  â”‚
   â”‚  [Chunk 87: To create an agent, use...] â”‚
   â”‚  [Chunk 201: Example code: @Agent...]    â”‚
   â”‚  ---                                     â”‚
   â”‚                                          â”‚
   â”‚  USER: What is Embabel and how...       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
7. LLM Generation
   [Ollama: incept5/Jan-v1-2509:fp16]
                â”‚
                â–¼
   "Embabel is an agent framework for the JVM
    created by Rod Johnson. Here's how to use it:
    
    1. Annotate your class with @Agent
    2. Define @Action methods for atomic tasks
    3. Define @Goal methods for high-level objectives
    
    Example:
    @Agent
    public class MyAgent {
        @Action
        public Data fetchData() { ... }
        
        @Goal
        public Report generateReport() { ... }
    }"
                â”‚
                â–¼
8. Response to User
   [Answer shown in terminal]
```

### What Makes This "Semantic" Search?

Traditional keyword search:
- **Query**: "How to make an agent"
- **Matches**: Exact words "make" and "agent"
- **Misses**: "create", "build", "construct" variations

Semantic vector search:
- **Query**: "How to make an agent"
- **Vector**: [0.123, -0.567, 0.890, ...]
- **Matches**: Documents with similar **meaning**:
  - "Creating your first agent"
  - "Agent construction guide"
  - "Build intelligent agents with @Agent"
- **Why**: Embeddings encode **semantic meaning**, not just words

### The Math: Cosine Similarity

```
Query Vector:    [0.8, 0.3, -0.5]
Document Vector: [0.7, 0.4, -0.4]

Cosine Similarity = (A Â· B) / (||A|| Ã— ||B||)
                  = (0.8Ã—0.7 + 0.3Ã—0.4 + -0.5Ã—-0.4) / (sqrt(...) Ã— sqrt(...))
                  = 0.89  â† High similarity (0-1 scale)

0.9-1.0 = Very similar (likely relevant)
0.7-0.9 = Similar (probably relevant)
0.5-0.7 = Somewhat similar (maybe relevant)
<0.5    = Different (likely not relevant)
```

### Why This Approach?

**Ingested Repositories** (configured in `repos.yaml`):
1. **Embabel Agent Framework** - Core agent patterns and @Agent/@Action/@Goal usage
2. **Embabel Examples** - Real-world agent implementations
3. **Embabel Java Template** - Starter project structure
4. **Embabel Kotlin Template** - Kotlin-specific patterns
5. **Tripper Application** - Production multi-agent system

**Result**: The agent can answer questions about:
- "How do I create an agent?" â†’ Finds @Agent examples
- "What's the difference between Action and Goal?" â†’ Finds conceptual docs
- "Show me a real example" â†’ Finds Tripper code
- "How does Spring AI integration work?" â†’ Finds integration guides

### Key Technical Details

**Embedding Model**: `nomic-embed-text`
- **Dimensions**: 1024 (higher = more nuanced meaning)
- **Context Length**: 8192 tokens (handles large chunks)
- **Speed**: ~50ms per embedding via Ollama
- **Why this model?**: Optimized for semantic search tasks

**Chunking Strategy**:
- **Size**: ~800 tokens (balance between context and precision)
- **Overlap**: 200 tokens (prevents splitting concepts)
- **Why overlap?**: Ensures code examples aren't cut mid-function

**Database**: PostgreSQL + pgvector
- **Storage**: Native vector column type
- **Indexing**: IVFFlat for fast approximate nearest neighbor search
- **Scale**: Handles millions of vectors efficiently
- **Why pgvector?**: Production-ready, ACID compliant, standard SQL

**Similarity Threshold**: 0.7 (configurable)
- **Higher (0.8-0.9)**: More precise, fewer results
- **Lower (0.5-0.7)**: More results, may include tangential info
- **Default (0.7)**: Good balance for documentation search

## Prerequisites

**IMPORTANT**: You must complete these steps before running the agent:

### 1. Docker Running
```bash
# Verify Docker is running
docker ps
```

### 2. Python 3.9+ Installed
```bash
# Check Python version
python3 --version  # Should be 3.9 or higher
```

### 4. Verify Python Embedding Service is Running

The Python service should already be running from the setup steps above.

**Check the service:**
```bash
# In the terminal where you ran ./start.sh, you should see:
# âœ“ Model loaded successfully (768 dimensions)
# INFO:     Uvicorn running on http://0.0.0.0:8001

# Test from another terminal:
curl http://localhost:8001/health
# Should return: {"status":"healthy","model":"nomic-ai/nomic-embed-text-v1.5","dimensions":768}
```

**If not running:**
```bash
cd stage-4-agentic-rag/embedding-service
./start.sh
```

**For troubleshooting**, see [`embedding-service/README.md`](./embedding-service/README.md)

### 5. Run the Ingestion Script (REQUIRED)
```bash
# In a new terminal
cd stage-4-agentic-rag

# This MUST be run before the agent will work
./ingest.sh
```

**What `ingest.sh` does**:
- âœ“ Verifies Python embedding service is running (REQUIRED)
- âœ“ Starts PostgreSQL with pgvector extension (Docker)
- âœ“ Runs database migrations (Flyway)
- âœ“ Loads repository text files (committed to git)
- âœ“ Chunks documents into searchable segments
- âœ“ Generates embeddings via Python service (768 dimensions)
- âœ“ Stores everything in PostgreSQL

**Optional refresh mode** (`./ingest.sh --refresh`):
- âœ“ Checks for `gitingest` installation (required for refresh)
- âœ“ Downloads fresh documentation from configured repositories
- âœ“ Processes as above with updated content

**Expected output**:
```
ğŸš€ Stage 4: RAG Ingestion Pipeline

ğŸ Using Python embedding service (REQUIRED)
âœ“ Python service is ready at http://localhost:8001

ğŸ“„ Using committed repository files from git
   (Use --refresh to fetch fresh data with gitingest)
ğŸ˜ Starting PostgreSQL with pgvector...
âœ“ PostgreSQL is ready
ğŸ”§ Running database migrations...
âœ“ Migrations complete
ğŸ“š Starting ingestion pipeline...

Processing embabel-agent...
  â†’ Using committed file: data/gitingest-output/embabel-agent.txt
  â†’ Chunks created: 98
  â†’ Embeddings generated: 98/98
  â†’ Stored: 98 documents

Processing embabel-examples...
  â†’ Using committed file: data/gitingest-output/embabel-examples.txt
  â†’ Chunks created: 67
  â†’ Embeddings generated: 67/67
  â†’ Stored: 67 documents
...

âœ… Ingestion pipeline complete!
Total documents: 487
```

**First-time setup takes**: 
- Python service first run: 2-3 minutes (download model, create environment)
- Ingestion: 2-3 minutes (generating embeddings from committed files)
- Total: ~5 minutes for complete setup

**Note**: The repository text files are already committed to git, so no external downloads are needed. To fetch fresh content, use `./ingest.sh --refresh` (requires `gitingest` installation).

## Repository Data Source

**Default Mode** (No External Dependencies):
- Repository text files are **committed to git** in `data/gitingest-output/`
- No need to install `gitingest` or download repositories
- Faster setup (2-3 minutes vs 5-10 minutes)
- Deterministic content (everyone gets identical files)

**Refresh Mode** (Optional):
```bash
./ingest.sh --refresh
```
- Fetches fresh repository content from GitHub
- Requires `gitingest` to be installed
- Useful for updating documentation or adding new repositories
- Takes 5-10 minutes (downloads + processes repositories)

**When to use refresh mode:**
- âœ… You want the latest documentation updates
- âœ… You've modified `repos.yaml` to add new repositories
- âœ… You're developing/testing the ingestion pipeline itself

**When to use default mode:**
- âœ… First-time workshop setup (faster, no dependencies)
- âœ… You just want to try the RAG agent
- âœ… Offline environment or slow network

## Quick Start

### Step 1: Setup (One-Time)

**Terminal 1 - Start Python Service:**
```bash
cd stage-4-agentic-rag/embedding-service
./start.sh

# Wait for:
# âœ“ Model loaded successfully (768 dimensions)
# INFO:     Uvicorn running on http://0.0.0.0:8001
```

**Terminal 2 - Run Ingestion:**
```bash
cd stage-4-agentic-rag

# Install dependencies and ingest documentation
./ingest.sh

# Verify ingestion succeeded
docker exec -it stage4-pgvector psql -U workshop -d workshop_rag -c "SELECT COUNT(*) FROM documents;"
# Should show: count: 487 (or similar number > 0)
```

### Step 2: Run the Agent

```bash
# Standard mode
./run.sh

# Verbose mode (see agent reasoning and tool calls)
./run.sh --verbose
```

### Step 3: Chat with the Agent

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  RAG Agent - Embabel & Spring AI Assistant        â•‘
â•‘  Type 'help' for commands, 'exit' to quit         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¬ You: What is Embabel?

[Agent thinking...]
[Tool: search_documentation]
[Retrieved 5 documents]

ğŸ¤– Assistant: 
Embabel is an agent framework for the JVM created by Rod Johnson,
the founder of Spring Framework. It uses Goal-Oriented Action Planning
(GOAP) to build intelligent applications.

Key features:
- @Agent annotation to define agent behavior
- Actions: Atomic steps an agent can take
- Goals: High-level objectives the agent works towards
- Automatic planning: Framework determines action sequences
- Integration with Spring AI

ğŸ’¬ You: Show me a simple example

[Using conversation context...]
[Tool: search_documentation]

ğŸ¤– Assistant:
Here's a basic Embabel agent example:

@Agent(description = "Quiz generator agent")
public class QuizAgent {
    
    @Action(description = "Fetch web content")
    public WebContent fetchContent(String url) {
        // Implementation
    }
    
    @Action(description = "Generate quiz from content")
    public Quiz generateQuiz(WebContent content) {
        // Implementation
    }
    
    @Goal
    public Quiz createQuizFromUrl(String url) {
        // Embabel will automatically plan:
        // 1. Call fetchContent(url)
        // 2. Call generateQuiz(content)
        return null;  // Placeholder for framework
    }
}

The framework automatically determines which actions to call
and in what order to achieve the goal.

ğŸ’¬ You: How does it integrate with Spring AI?

[Continuing conversation...]
```

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAGAgent.chat()              â”‚
â”‚  â€¢ Add to ConversationMemory         â”‚
â”‚  â€¢ Build prompt with history         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Agent Loop (Max 10 iterations)  â”‚
â”‚  1. Think: LLM processes prompt      â”‚
â”‚  2. Act: Calls tool (if needed)      â”‚
â”‚  3. Observe: Processes tool result   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â†’ Tool Call? â”€â”€Noâ”€â”€â†’ Final Answer
       â”‚                            â†“
       Yes                   Return to User
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      search_documentation            â”‚
â”‚  â€¢ Parse JSON tool call              â”‚
â”‚  â€¢ Execute vector search             â”‚
â”‚  â€¢ Expand with neighbors (optional)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PgVectorStore.search()            â”‚
â”‚  â€¢ Generate query embedding          â”‚
â”‚  â€¢ Cosine similarity search          â”‚
â”‚  â€¢ Return top K results              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL + pgvector               â”‚
â”‚  â€¢ 487 documents ingested            â”‚
â”‚  â€¢ 1024-dim embeddings               â”‚
â”‚  â€¢ IVFFlat index for fast search     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
   Retrieved Documents
       â”‚
       â–¼
   Add to Memory & Continue Loop
```

## Key Features

### 1. Two-Phase Architecture

**Phase 1: Ingestion (Shell Script)**
- Runs once to populate the database
- Can be re-run to update documentation
- Hash-based change detection (only processes new content)

**Phase 2: Conversational Agent (Java)**
- Interactive Q&A over ingested docs
- Maintains conversation context
- Retrieves relevant information on-demand

### 2. Multi-Turn Conversations

The agent remembers previous exchanges:

```
Q1: "What is Embabel?"
A1: [Explains Embabel framework]

Q2: "Who created it?"          â† Uses context from Q1
A2: "Rod Johnson, founder of Spring Framework"

Q3: "Show me an example"       â† Understands "it" refers to Embabel
A3: [Provides code example]
```

### 3. Context Expansion

When retrieving documents, the agent can fetch neighboring chunks:

```java
// If document is part of a larger explanation
if (expandContext) {
    // Get previous and next chunks from same file
    documents = expandWithNeighbors(documents);
}
```

This ensures complete code examples and maintains narrative flow.

### 4. JSON Tool Calling

Cleaner than XML format (used in Stage 1):

```json
{
  "tool": "search_documentation",
  "parameters": {
    "query": "how to create an agent",
    "topK": 5,
    "expandContext": true
  }
}
```

## CLI Commands

Once the agent is running, you can use these commands:

- `help` - Show available commands
- `history` - Display full conversation history
- `clear` - Clear conversation memory (start fresh)
- `exit` or `quit` - End conversation

## Testing

### Quick Tool Calling Test

Verify that tool calling works correctly:

```bash
./test-tool-calling.sh
```

This script will:
1. Check all prerequisites (Ollama, models, PostgreSQL, documents)
2. Run a simple query with verbose output
3. Verify that the agent correctly parses and executes tool calls

**Look for:**
- `âœ“ Parsed as tool call: ToolCall{...}` â† Tool calling works!
- `âœ— Not a tool call` â† May indicate parsing issue

### Integration Tests

```bash
# Build and run all tests
mvn test

# Run specific test
mvn test -Dtest=RAGAgentIntegrationTest#testVectorSearch
```

**Test Coverage:**
- âœ… Vector search with similarity threshold
- âœ… Single-turn conversation
- âœ… Multi-turn conversation with context
- âœ… Tool invocation and parsing
- âœ… Context expansion with neighbors
- âœ… Conversation memory management
- âœ… Database connection and queries

**Note**: First test run may be slow (30-60 seconds) while Ollama loads the model into memory.

## Project Structure

```
stage-4-agentic-rag/
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ architecture.md                  # Detailed architecture documentation
â”œâ”€â”€ ingest.sh                        # âš ï¸ RUN THIS FIRST
â”œâ”€â”€ run.sh                           # Run the agent
â”œâ”€â”€ test-tool-calling.sh             # Quick verification script
â”œâ”€â”€ cleanup.sh                       # Remove database and generated files
â”œâ”€â”€ docker-compose.yml               # PostgreSQL + pgvector setup
â”œâ”€â”€ repos.yaml                       # Configure what to ingest
â”‚
â”œâ”€â”€ db/migration/
â”‚   â””â”€â”€ V1__Create_documents_table.sql   # Database schema (Flyway)
â”‚
â”œâ”€â”€ data/                            # Generated by ingest.sh (gitignored)
â”‚   â””â”€â”€ gitingest-output/
â”‚       â”œâ”€â”€ spring-ai.txt
â”‚       â”œâ”€â”€ embabel-agent.txt
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ main/java/com/incept5/workshop/stage4/
    â”‚   â”œâ”€â”€ agent/
    â”‚   â”‚   â”œâ”€â”€ RAGAgent.java                # Main conversational agent
    â”‚   â”‚   â”œâ”€â”€ ConversationMemory.java      # Multi-turn context tracking
    â”‚   â”‚   â””â”€â”€ RAGAgentDemo.java            # Interactive CLI
    â”‚   â”œâ”€â”€ tool/
    â”‚   â”‚   â”œâ”€â”€ Tool.java                    # Tool interface
    â”‚   â”‚   â”œâ”€â”€ ToolRegistry.java            # Tool management
    â”‚   â”‚   â””â”€â”€ RAGTool.java                 # Document search with expansion
    â”‚   â”œâ”€â”€ util/
    â”‚   â”‚   â””â”€â”€ JsonToolCallParser.java      # Parse JSON tool calls
    â”‚   â”œâ”€â”€ db/
    â”‚   â”‚   â”œâ”€â”€ Document.java                # Document record with chunk info
    â”‚   â”‚   â”œâ”€â”€ PgVectorStore.java           # Vector database operations
    â”‚   â”‚   â””â”€â”€ DatabaseConfig.java          # HikariCP connection pool
    â”‚   â””â”€â”€ ingestion/
    â”‚       â”œâ”€â”€ IngestionService.java        # Main ingestion orchestration
    â”‚       â”œâ”€â”€ DocumentChunker.java         # Split docs into chunks
    â”‚       â”œâ”€â”€ EmbeddingService.java        # Generate embeddings
    â”‚       â”œâ”€â”€ IngestionConfig.java         # Configuration
    â”‚       â””â”€â”€ RepoConfig.java              # Repository metadata
    â”‚
    â””â”€â”€ test/java/com/incept5/workshop/stage4/
        â”œâ”€â”€ RAGAgentIntegrationTest.java     # Comprehensive tests
        â””â”€â”€ VectorSearchIntegrationTest.java # Database-specific tests
```

## What Makes This Different from Stage 1?

| Aspect | Stage 1 (Simple Agent) | Stage 3 (RAG Agent) |
|--------|------------------------|---------------------|
| **Memory** | None (single turn) | Full conversation history |
| **Tool Format** | XML (custom) | JSON (Ollama native) |
| **Data Source** | External APIs (weather, country) | Vector database (docs) |
| **Context** | Single question | Multi-turn dialogue |
| **Scale** | 2 tools, instant calls | 1 RAG tool, 487+ documents |
| **Expansion** | N/A | Neighboring chunk retrieval |
| **Setup** | None required | Ingestion pipeline required |

## Configuration

### Changing Models

Edit `RAGAgentDemo.java`:

```java
private static final String LLM_MODEL = "incept5/Jan-v1-2509:fp16";
private static final String EMBEDDING_MODEL = "nomic-embed-text";
```

### Adding More Repositories

Edit `repos.yaml`:

```yaml
repositories:
  - name: my-repo
    url: https://github.com/user/my-repo
    branch: main
    description: "My custom repository"
```

Then re-run `./ingest.sh` to add the new content.

### Adjusting Chunk Size

Edit `repos.yaml`:

```yaml
settings:
  chunk_size: 800          # tokens per chunk (default)
  chunk_overlap: 200       # overlap for context (default)
  similarity_threshold: 0.7  # minimum cosine similarity (default)
```

## Troubleshooting

### "No documents found" error

**Solution**: Run the ingestion script
```bash
./ingest.sh
```

**Verify**:
```bash
docker exec -it stage4-pgvector psql -U workshop -d workshop_rag -c "SELECT COUNT(*) FROM documents;"
```

Should show count > 0 (typically ~487 documents).

### Ollama connection refused (for LLM)

**Solution**: Start Ollama and pull the LLM model
```bash
# Start Ollama
ollama serve

# Pull required LLM model (for agent reasoning)
ollama pull incept5/Jan-v1-2509:fp16

# Verify
curl http://localhost:11434/api/tags
```

**Note**: You do NOT need `nomic-embed-text` in Ollama. The Python service handles embeddings.

### PostgreSQL connection refused

**Solution**: Start Docker Compose
```bash
cd stage-4-agentic-rag
docker-compose up -d

# Check status
docker-compose ps

# View logs if needed
docker-compose logs
```

### Slow first response (30-60 seconds)

**Explanation**: This is normal! Ollama loads the model into memory on the first request.

**Subsequent queries are much faster** (1-5 seconds typically).

**Alternative**: Use a smaller/faster model:
```bash
ollama pull qwen2.5:7b
# Update LLM_MODEL in RAGAgentDemo.java
```

### Python embedding service not running

**Error**: "Connection refused" or "Service not available" during ingestion

**Solution**: Start the Python service
```bash
cd embedding-service
./start.sh

# Wait for:
# âœ“ Model loaded successfully (768 dimensions)
# INFO:     Uvicorn running on http://0.0.0.0:8001
```

**Verify it's working:**
```bash
curl http://localhost:8001/health
# Should return: {"status":"healthy",...}
```

### Python service setup issues

**Problem**: Service won't start or shows errors

**Solution 1**: Run verification script
```bash
cd embedding-service
./verify-setup.sh
```

**Solution 2**: Recreate conda environment
```bash
conda env remove -n embedding-service
./start.sh  # Will recreate environment
```

**Solution 3**: Check detailed troubleshooting
See [`embedding-service/README.md`](./embedding-service/README.md) for:
- Conda installation issues
- Python version problems
- Module not found errors
- Port conflicts

### "Repository file not found" error

**Cause**: Text files missing from `data/gitingest-output/`

**Solution 1**: Ensure files are checked out from git
```bash
git status
git checkout data/gitingest-output/*.txt
```

**Solution 2**: Fetch fresh content with gitingest
```bash
# Install gitingest if not available
brew install pipx && pipx install gitingest  # macOS
# or
pipx install gitingest  # if pipx already installed

# Run ingestion in refresh mode
./ingest.sh --refresh
```

### Clean Slate (Reset Everything)

```bash
# Stop database and remove all data
./cleanup.sh

# Or manually:
docker-compose down -v
rm -rf data/

# Stop Python service (Ctrl+C in the terminal where it's running)
# Or:
pkill -f "python.*server.py"
```

Then restart:
1. Python service: `cd embedding-service && ./start.sh`
2. Ingestion: `./ingest.sh`

## What You'll Learn

By completing this stage, you'll understand:

âœ… **RAG Fundamentals**: Retrieval â†’ Augmentation â†’ Generation flow  
âœ… **Vector Databases**: PostgreSQL + pgvector for production use  
âœ… **Embeddings**: How to generate and use semantic vectors  
âœ… **Conversational AI**: Multi-turn dialogues with memory  
âœ… **Document Processing**: Chunking strategies and overlap  
âœ… **Tool Integration**: JSON-based tool calling with Ollama  
âœ… **Production Patterns**: Connection pooling, migrations, idempotency  

## Next Steps

After completing Stage 3, you're ready for:

- **Stage 4**: Multi-Agent Teams (orchestration, specialized agents)
- **Stage 5**: Enterprise Patterns (monitoring, resilience, security)

Or explore enhancements:
- Add metadata filtering to vector search
- Implement hybrid search (vector + keyword)
- Add document re-ranking for better results
- Create a web UI with streaming responses
- Add persistent conversation storage

## Resources

### Internal Documentation
- **Root Architecture**: [/architecture.md](../architecture.md)
- **Stage 3 Architecture**: [architecture.md](./architecture.md)
- **Stage 1 (Simple Agent)**: [/stage-1-simple-agent/README.md](../stage-1-simple-agent/README.md)

### External Resources
- [Spring AI RAG Documentation](https://docs.spring.io/spring-ai/reference/api/retrieval-augmented-generation.html)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [gitingest GitHub](https://github.com/cyclotruc/gitingest)
- [Ollama Embeddings API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings)
- [nomic-embed-text Model](https://ollama.com/library/nomic-embed-text)

---

**Stage 3 Status**: âœ… Complete  
**Last Updated**: 2025-11-06  
**Total Code**: ~1,800 lines across 15 files  
**Dependencies**: Docker, Ollama, PostgreSQL, Java 21+

---
