{
  "filePath": "src/main/java/com/incept5/ollama/backend/MLXVLMBackend.java",
  "mtime": 1762426412327,
  "size": 7104,
  "definitions": [
    "public class MLXVLMBackend extends AbstractHttpBackend {",
    "public AIResponse generate(String prompt, String systemPrompt, Map<String, Object> options)",
    "public AIResponse generateStreaming(",
    "public CompletableFuture<AIResponse> generateAsync(",
    "private AIResponse sendRequest(MLXVLMRequest request) throws AIBackendException {",
    "private AIResponse sendStreamingRequest(MLXVLMRequest request, Consumer<String> chunkConsumer)",
    "private AIResponse parseResponse(String jsonResponse) throws AIBackendException {",
    "public ModelInfo getModelInfo(String modelName) throws AIBackendException {",
    "public BackendType getBackendType() {",
    "public boolean supportsModelInfo() {"
  ],
  "cachedAt": 1762428058508,
  "language": "java"
}